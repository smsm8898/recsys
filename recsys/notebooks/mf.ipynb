{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49f2e00-2159-4081-91df-a8417ee4019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(\"/Users/seungminjang/Desktop/workspace/recsys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd4f837-95a4-40d4-aaf3-610321edb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, sparse_feature_names):\n",
    "        self.sparse_feature_names = sparse_feature_names\n",
    "        self.data = data[:, :-1]\n",
    "        self.label = data[:, [-1]].astype(float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            self.sparse_feature_names[0]: self.data[idx, 0],\n",
    "            self.sparse_feature_names[1]: self.data[idx, 1],\n",
    "        }, self.label[idx]\n",
    "        \n",
    "class MovielensMatrixFactorization(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_sparse_features: dict[str, int],\n",
    "        latent_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_sparse_features = num_sparse_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sparse_feature_names = list(num_sparse_features.keys())\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleDict({\n",
    "            name : torch.nn.Embedding(num_sparse_feature, latent_dim)\n",
    "            for name, num_sparse_feature in num_sparse_features.items()\n",
    "        })\n",
    "        \n",
    "    def forward(self, sparse_features: dict[str, torch.LongTensor]) -> torch.FloatTensor:\n",
    "        emb1 = self.embeddings[self.sparse_feature_names[0]](sparse_features[self.sparse_feature_names[0]])\n",
    "        emb2 = self.embeddings[self.sparse_feature_names[1]](sparse_features[self.sparse_feature_names[1]])\n",
    "        logits = (emb1 * emb2).sum(dim=1, keepdim=True)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5f47ff-edf1-47a2-866b-0f00040d9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"movielens/raw/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "users = pd.read_csv(\"movielens/raw/u.user\", sep=\"|\", encoding=\"latin-1\", names=[\"user_id\", \"age\", \"gender\", \"occupation\", \"zip\"])\n",
    "items = pd.read_csv(\"movielens/raw/u.item\", sep=\"|\", encoding=\"latin-1\", names=[\n",
    "        \"item_id\",\n",
    "        \"title\",\n",
    "        \"release_date\",\n",
    "        \"video_release_date\",\n",
    "        \"imdb_url\",\n",
    "        \"unknown\",\n",
    "        \"action\",\n",
    "        \"adventure\",\n",
    "        \"animation\",\n",
    "        \"children\",\n",
    "        \"comedy\",\n",
    "        \"crime\",\n",
    "        \"documentary\", \n",
    "        \"drama\", \n",
    "        \"fantasy\",\n",
    "        \"film-noir\",\n",
    "        \"horror\",\n",
    "        \"musical\",\n",
    "        \"mystery\",\n",
    "        \"romance\",\n",
    "        \"sci-Fi\",\n",
    "        \"thriller\",\n",
    "        \"war\",\n",
    "        \"western\",\n",
    "    ])\n",
    "num_sparse_features = {\n",
    "    \"user_id\": users[\"user_id\"].nunique(),\n",
    "    \"item_id\": items[\"item_id\"].nunique(),\n",
    "}\n",
    "vocabulary = {\n",
    "    \"user_id\": np.unique(users[\"user_id\"]),\n",
    "    \"item_id\": np.unique(items[\"item_id\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f208338d-3422-485d-bfd3-34023eb09d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제된 알 수 없는 데이터: 0\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "processed = ratings.copy()\n",
    "\n",
    "# user_id, item_id 정보가 있는 데이터만 남기기\n",
    "processed = processed[(\n",
    "    processed[\"user_id\"].isin(vocabulary[\"user_id\"])\n",
    "    & processed[\"item_id\"].isin(vocabulary[\"item_id\"])\n",
    ")]\n",
    "unknown = len(ratings) - len(processed)\n",
    "print(\"삭제된 알 수 없는 데이터:\", unknown)\n",
    "\n",
    "\n",
    "# user_id, item_id를 index 매핑\n",
    "processed[\"user_id\"] = processed[\"user_id\"].map({\n",
    "    uid : i \n",
    "    for i, uid in enumerate(vocabulary[\"user_id\"])\n",
    "})\n",
    "\n",
    "processed[\"item_id\"] = processed[\"item_id\"].map({\n",
    "    iid : i \n",
    "    for i, iid in enumerate(vocabulary[\"item_id\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d43775-2a1e-43aa-b7e7-ab5c2932ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 943/943 [00:00<00:00, 2466.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset Hyperparameter\n",
    "maximum_positive_sample = 100\n",
    "num_negative_sample = 5\n",
    "all_item_indices = np.arange(num_sparse_features[\"item_id\"])\n",
    "\n",
    "# Leave-one-out split\n",
    "# Downsampling\n",
    "# Random Negative Sampling\n",
    "train, test = [], []\n",
    "for u, group in tqdm(processed.sort_values(by=\"timestamp\").groupby(\"user_id\")):\n",
    "    # Leave one\n",
    "    last = group.iloc[-1, :2].values\n",
    "    test.append([last[0], last[1], 1])\n",
    "    \n",
    "    \n",
    "    # Downsampling\n",
    "    group = group.iloc[:-1]\n",
    "    num_positive_sample = len(group)\n",
    "    num_positive_sample = min(maximum_positive_sample, num_positive_sample)\n",
    "    group = group.tail(num_positive_sample)\n",
    "\n",
    "    # positive samples\n",
    "    positive = group[[\"user_id\", \"item_id\"]].values\n",
    "    positive = np.column_stack([positive, np.ones(len(positive), dtype=int)])\n",
    "\n",
    "    # negative sampling\n",
    "    positive_item_indices = group[\"item_id\"].unique()\n",
    "    negative_item_indices = np.setdiff1d(all_item_indices, positive_item_indices)\n",
    "\n",
    "    size = num_positive_sample*num_negative_sample\n",
    "    replace_flag = size > len(negative_item_indices)\n",
    "    negative_item_indices = np.random.choice(\n",
    "        negative_item_indices,\n",
    "        size=size,\n",
    "        replace=replace_flag\n",
    "    )\n",
    "\n",
    "    negative = np.column_stack([\n",
    "        np.full(size, u, dtype=int),\n",
    "        negative_item_indices,\n",
    "        np.zeros(size, dtype=int)\n",
    "    ])\n",
    "    train.append(positive)\n",
    "    train.append(negative)\n",
    "\n",
    "train = np.vstack(train)\n",
    "test = np.vstack(test)\n",
    "\n",
    "# save in processed\n",
    "np.save(\"movielens/processed/train.npy\", train)\n",
    "np.save(\"movielens/processed/test.npy\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b1fcf0-ddd3-45f2-aacf-4a8e5a6f5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameter\n",
    "batch_size = 64\n",
    "latent_dim = 16\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "\n",
    "# Define Dataset\n",
    "train_ds = MovielensDataset(train, list(num_sparse_features.keys()))\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = MovielensMatrixFactorization(num_sparse_features, latent_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76d11ed-1e00-4e76-a6e9-1dc81b1d7b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█| 5747/5747 [00:08<00:00, 649.58it/s, auc=0.0000, logloss=1.72\n",
      "Epoch 2/20: 100%|█| 5747/5747 [00:09<00:00, 617.30it/s, auc=0.4996, logloss=1.46\n",
      "Epoch 3/20: 100%|█| 5747/5747 [00:10<00:00, 572.90it/s, auc=0.5101, logloss=1.24\n",
      "Epoch 4/20: 100%|█| 5747/5747 [00:09<00:00, 589.49it/s, auc=0.5237, logloss=1.06\n",
      "Epoch 5/20: 100%|█| 5747/5747 [00:09<00:00, 578.05it/s, auc=0.5421, logloss=0.89\n",
      "Epoch 6/20: 100%|█| 5747/5747 [00:10<00:00, 561.75it/s, auc=0.5696, logloss=0.71\n",
      "Epoch 7/20: 100%|█| 5747/5747 [00:09<00:00, 578.09it/s, auc=0.6114, logloss=0.53\n",
      "Epoch 8/20: 100%|█| 5747/5747 [00:09<00:00, 577.17it/s, auc=0.6660, logloss=0.37\n",
      "Epoch 9/20: 100%|█| 5747/5747 [00:10<00:00, 545.96it/s, auc=0.7223, logloss=0.27\n",
      "Epoch 10/20: 100%|█| 5747/5747 [00:10<00:00, 572.06it/s, auc=0.7701, logloss=0.2\n",
      "Epoch 11/20: 100%|█| 5747/5747 [00:11<00:00, 519.06it/s, auc=0.8051, logloss=0.2\n",
      "Epoch 12/20: 100%|█| 5747/5747 [00:12<00:00, 445.86it/s, auc=0.8283, logloss=0.1\n",
      "Epoch 13/20: 100%|█| 5747/5747 [00:10<00:00, 540.73it/s, auc=0.8433, logloss=0.1\n",
      "Epoch 14/20: 100%|█| 5747/5747 [00:10<00:00, 539.16it/s, auc=0.8536, logloss=0.1\n",
      "Epoch 15/20: 100%|█| 5747/5747 [00:10<00:00, 533.32it/s, auc=0.8615, logloss=0.1\n",
      "Epoch 16/20: 100%|█| 5747/5747 [00:10<00:00, 547.23it/s, auc=0.8680, logloss=0.1\n",
      "Epoch 17/20: 100%|█| 5747/5747 [00:10<00:00, 546.45it/s, auc=0.8737, logloss=0.1\n",
      "Epoch 18/20: 100%|█| 5747/5747 [00:10<00:00, 525.42it/s, auc=0.8790, logloss=0.1\n",
      "Epoch 19/20: 100%|█| 5747/5747 [00:10<00:00, 535.47it/s, auc=0.8839, logloss=0.1\n",
      "Epoch 20/20: 100%|█| 5747/5747 [00:10<00:00, 526.81it/s, auc=0.8886, logloss=0.1\n"
     ]
    }
   ],
   "source": [
    "# Train MF\n",
    "history = defaultdict(list)\n",
    "model.train()\n",
    "auc = 0\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for sparse_features, labels in pbar:\n",
    "        sparse_features = {k: v.to(device) for k, v in sparse_features.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        \n",
    "        # forward\n",
    "        logits = model(sparse_features)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # metrics\n",
    "        total_loss += loss.item()\n",
    "        probs = logits.sigmoid().detach().cpu().numpy()\n",
    "        all_preds.extend(probs.flatten())\n",
    "        all_labels.extend(labels.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        # tqdm update\n",
    "        pbar.set_postfix(\n",
    "            logloss=loss.item(),\n",
    "            auc=f\"{auc:.4f}\"\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = ((np.array(all_preds) > 0.5) == np.array(all_labels)).mean()\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    history[\"accuracy\"].append(acc)\n",
    "    history[\"auroc\"].append(auc)\n",
    "    history[\"logloss\"].append(avg_loss)\n",
    "    \n",
    "    pbar.set_postfix(\n",
    "        logloss=f\"{avg_loss:.4f}\",\n",
    "        auc=f\"{auc:.4f}\"\n",
    "    )\n",
    "    pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff35620-4791-4484-825b-bbf9d3571748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TopK Recommend: 100%|██████████| 943/943 [00:00<00:00, 1455.21it/s, user_id=942]\n"
     ]
    }
   ],
   "source": [
    "# TopK recommendation\n",
    "topk = 10\n",
    "\n",
    "user_recommendations = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_item_indices = torch.arange(num_sparse_features[\"item_id\"]).to(device)\n",
    "    pbar = tqdm(range(num_sparse_features[\"user_id\"]), desc=\"TopK Recommend\")\n",
    "    for user_id in pbar:\n",
    "        sparse_features = {\n",
    "            \"user_id\":torch.full(size=(len(all_item_indices),), fill_value=user_id).to(device),\n",
    "            \"item_id\":all_item_indices,\n",
    "        }\n",
    "        logits = model(sparse_features)\n",
    "        indices = logits.flatten().argsort(descending=True)\n",
    "        \n",
    "\n",
    "        user_recommendations[user_id] = indices.detach().cpu().numpy()\n",
    "\n",
    "        pbar.set_postfix(user_id=user_id)\n",
    "        \n",
    "# 대조군(Popular)\n",
    "test_data = processed.sort_values(by=\"timestamp\").groupby(\"user_id\").tail(1)\n",
    "train_data = processed.drop(test_data.index)    \n",
    "popular_recommendations = train_data[\"item_id\"].value_counts().index.values\n",
    "\n",
    "# 대조군(Random)\n",
    "all_item_indices = all_item_indices.numpy()\n",
    "random_recommendations = np.random.choice(all_item_indices, size=len(all_item_indices), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4b4448-c791-47b6-b8df-e74f02a5eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hit_ratio_at_k(recs: np.ndarray, real: int, k: int) -> float:\n",
    "    return 1.0 if real in recs[:k] else 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(recs: np.ndarray, real: int, k: int) -> float:\n",
    "    recs_k = recs[:k]\n",
    "    idx = np.where(recs_k == real)[0]\n",
    "    if len(idx) > 0:\n",
    "        rank = idx[0] + 1  # 1-based\n",
    "        return 1.0 / np.log2(rank + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def mrr_at_k(recs: np.ndarray, real: int, k: int) -> float:\n",
    "    recs_k = recs[:k]\n",
    "    idx = np.where(recs_k == real)[0]\n",
    "    if len(idx) > 0:\n",
    "        rank = idx[0] + 1\n",
    "        return 1.0 / rank\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eed2fe3-62ef-4c7c-a93b-fee19d16d389",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         mrr[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopular@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mrr_at_k(popular_recommendations, item_id, k)\n\u001b[1;32m     22\u001b[0m         mrr[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mrr_at_k(random_recommendations, item_id, k)\n\u001b[0;32m---> 24\u001b[0m hr \u001b[38;5;241m=\u001b[39m {k: np\u001b[38;5;241m.\u001b[39mround(v\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data), \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhit\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m ndcg \u001b[38;5;241m=\u001b[39m {k: np\u001b[38;5;241m.\u001b[39mround(v\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data), \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m ndcg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     26\u001b[0m mrr \u001b[38;5;241m=\u001b[39m {k: np\u001b[38;5;241m.\u001b[39mround(v\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data), \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m mrr\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hit' is not defined"
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 15, 20]\n",
    "hr = defaultdict(float)\n",
    "ndcg = defaultdict(float)\n",
    "mrr = defaultdict(float)\n",
    "for _, line in test_data.iterrows():\n",
    "    user_id, item_id = line.user_id, line.item_id\n",
    "    \n",
    "    recs = user_recommendations[user_id]\n",
    "\n",
    "    # calculate\n",
    "    for k in ks:\n",
    "        hr[f\"mf@{k}\"] += hit_ratio_at_k(recs, item_id, k)\n",
    "        hr[f\"popular@{k}\"] += hit_ratio_at_k(popular_recommendations, item_id, k)\n",
    "        hr[f\"random@{k}\"] += hit_ratio_at_k(random_recommendations, item_id, k)\n",
    "    \n",
    "        ndcg[f\"mf@{k}\"] += ndcg_at_k(recs, item_id, k)\n",
    "        ndcg[f\"popular@{k}\"] += ndcg_at_k(popular_recommendations, item_id, k)\n",
    "        ndcg[f\"random@{k}\"] += ndcg_at_k(random_recommendations, item_id, k)\n",
    "    \n",
    "        mrr[f\"mf@{k}\"] += mrr_at_k(recs, item_id, k)\n",
    "        mrr[f\"popular@{k}\"] += mrr_at_k(popular_recommendations, item_id, k)\n",
    "        mrr[f\"random@{k}\"] += mrr_at_k(random_recommendations, item_id, k)\n",
    "    \n",
    "hr = {k: np.round(v/len(test_data), 4) for k,v in hit.items()}\n",
    "ndcg = {k: np.round(v/len(test_data), 4) for k,v in ndcg.items()}\n",
    "mrr = {k: np.round(v/len(test_data), 4) for k,v in mrr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7feb3-caaf-4d1c-8758-4a547bc2adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\"HR\": hr, \"NDCG\": ndcg, \"MRR\": mrr}\n",
    "models = [\"mf\", \"popular\", \"random\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "\n",
    "for ax, (metric_name, metric_dict) in zip(axes, metrics.items()):\n",
    "    for model in models:\n",
    "        values = [metric_dict[f\"{model}@{k}\"] for k in ks]\n",
    "        ax.plot(ks, values, marker=\"o\", label=model)\n",
    "\n",
    "    ax.set_title(metric_name)\n",
    "    ax.set_xlabel(\"K\")\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859a844-55c7-4237-a85e-eb13a5b3efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1801fc-0653-4e0a-9ebd-3a7508a6b62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf517f-6252-4ef2-9d71-b31a0d2d7de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
